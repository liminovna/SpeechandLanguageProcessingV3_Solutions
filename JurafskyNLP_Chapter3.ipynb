{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCxKvliTIYSq5+4J0r9s+q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liminovna/SpeechandLanguageProcessingV3_Solutions/blob/main/JurafskyNLP_Chapter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Write out the equation for trigram probability estimation"
      ],
      "metadata": {
        "id": "59Ruee-hoZyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$P(w_n|w_{n-2}w_{n-1}) = \\frac{C(w_{n-2}w_{n-1}w_n)}{C(w_{n-2}w_{n-1})}$"
      ],
      "metadata": {
        "id": "UHXuOT9NkyPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now write out all the non-zero trigram probabilities for the *I am Sam* corpus\n",
        "\n",
        "    <s> I am Sam </s>\n",
        "    <s> Sam I am </s>\n",
        "    <s> I do not like green eggs and ham </s>"
      ],
      "metadata": {
        "id": "T8z8qZb1mHol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrigramModel():\n",
        "  def __init__(self, corpus):\n",
        "    # self.unigrams = {}\n",
        "    self.bigrams = {}\n",
        "    self.trigrams = {}\n",
        "\n",
        "    for s in corpus:\n",
        "      words = s.lower().split()\n",
        "      # # appending unigrams\n",
        "      # for w in words:\n",
        "      #   if w not in self.unigrams.keys():\n",
        "      #     self.unigrams[w] = 1\n",
        "      #   else:\n",
        "      #     self.unigrams[w] += 1\n",
        "      # appending bigrams\n",
        "      cntr = 0\n",
        "      for i in range(len(words)-1):\n",
        "        cur_bigram = (words[cntr] + ' ' + words[cntr+1])\n",
        "        if cur_bigram not in self.bigrams:\n",
        "          self.bigrams[cur_bigram] = 1\n",
        "        else:\n",
        "          self.bigrams[cur_bigram] += 1\n",
        "        cntr += 1\n",
        "      # appending trigrams\n",
        "      cntr = 0\n",
        "      for i in range(len(words)-2):\n",
        "        cur_trigram = (words[cntr] + ' ' + words[cntr+1] + ' ' + words[cntr+2])\n",
        "        if cur_trigram not in self.trigrams:\n",
        "          self.trigrams[cur_trigram] = 1\n",
        "        else:\n",
        "          self.trigrams[cur_trigram] += 1\n",
        "        cntr += 1\n",
        "\n",
        "  # def add1smoothing_proba(self, s):\n",
        "  #   s = s.lower()\n",
        "  #   words = s.split()\n",
        "  #   return (self.trigrams[s] + 1) / (self.unigrams[words[0]]+len(self.unigrams))\n",
        "\n",
        "  def unsmoothed_proba(self, s):\n",
        "    s = s.lower()\n",
        "    words = s.split()\n",
        "    return self.trigrams[s]/self.bigrams[words[0] + ' ' + words[1]]"
      ],
      "metadata": {
        "id": "0UL8k6V1oYr5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    '<s> I am Sam </s>',\n",
        "    '<s> Sam I am </s>',\n",
        "    '<s> I do not like green eggs and ham </s>',\n",
        "]\n",
        "\n",
        "model = TrigramModel(corpus=corpus)"
      ],
      "metadata": {
        "id": "NWONzZD_ojNu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Answer: ')\n",
        "for t in model.trigrams.keys():\n",
        "  print(f'P({t}) = {model.unsmoothed_proba(t)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkItmVP9qQqs",
        "outputId": "5ff3e256-6f8e-445e-f0cd-5d2b72c7249e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: \n",
            "P(<s> i am) = 0.5\n",
            "P(i am sam) = 0.5\n",
            "P(am sam </s>) = 1.0\n",
            "P(<s> sam i) = 1.0\n",
            "P(sam i am) = 1.0\n",
            "P(i am </s>) = 0.5\n",
            "P(<s> i do) = 0.5\n",
            "P(i do not) = 1.0\n",
            "P(do not like) = 1.0\n",
            "P(not like green) = 1.0\n",
            "P(like green eggs) = 1.0\n",
            "P(green eggs and) = 1.0\n",
            "P(eggs and ham) = 1.0\n",
            "P(and ham </s>) = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Calculate the probability of the sentence *i want chinese food*. Give two probabilities -- unsmoothed and an add-one probability. Assume the additional add-1 smoothed probabilities $P(i|<s>)=0.19$ and $P(</s>|food)=0.40$."
      ],
      "metadata": {
        "id": "cku9pk3nqyMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = '<s> I want chinese food <\\s>'\n",
        "unsmoothed_proba = 0.25*0.33*0.0065*0.52*0.68\n",
        "addone_proba = 0.19*0.21*0.0029*0.52*0.4\n",
        "print('Answer\\n',\\\n",
        "      'Unsmoothed probability:', \"{:.5f}\".format(unsmoothed_proba), '\\n',\\\n",
        "      'Add-one probability:', \"{:.7f}\".format(addone_proba))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AobKG_ylquZ1",
        "outputId": "95a6f470-16d9-4ff8-b373-9a12d40d006d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer\n",
            " Unsmoothed probability: 0.00019 \n",
            " Add-one probability: 0.0000241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Which of the two probabilities you computed in the previous exercise is higher, unsmoothed or smoothed? Explain why."
      ],
      "metadata": {
        "id": "m8DG8qbgrR97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "The unsmoothed probability is higher because we did not give away any bit of it to the events we have not seen, unlike smoothed probability which is what's left after we've 'shaved off' some of the probability mass."
      ],
      "metadata": {
        "id": "MfjiptJzrVWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 We are given the following corpus:\n",
        "\n",
        "    '<s> i am sam </s>',\n",
        "    '<s> sam i am </s>',\n",
        "    '<s> i am sam </s>',\n",
        "    '<s> i do not like green eggs and sam </s>',\n",
        "\n",
        "Using a bigram language model with add-one smoothing, what is P(Sam|am)? Include \\<s> and \\<\\s> in your counts just like any other token."
      ],
      "metadata": {
        "id": "5WeQvLshhizt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramModel():\n",
        "  def __init__(self, corpus):\n",
        "    self.unigrams = {}\n",
        "    self.bigrams = {}\n",
        "\n",
        "    for s in corpus:\n",
        "      words = s.split()\n",
        "      # appending unigrams\n",
        "      for w in words:\n",
        "        if w not in self.unigrams.keys():\n",
        "          self.unigrams[w] = 1\n",
        "        else:\n",
        "          self.unigrams[w] += 1\n",
        "      # appending bigrams\n",
        "      cntr = 0\n",
        "      for i in range(len(words)-1):\n",
        "        cur_bigram = (words[cntr] + ' ' + words[cntr+1])\n",
        "        if cur_bigram not in self.bigrams:\n",
        "          self.bigrams[cur_bigram] = 1\n",
        "        else:\n",
        "          self.bigrams[cur_bigram] += 1\n",
        "        cntr += 1\n",
        "\n",
        "  def add1smoothing_proba(self, s):\n",
        "    s = s.lower()\n",
        "    words = s.split()\n",
        "    return (self.bigrams[s] + 1) / (self.unigrams[words[0]]+len(self.unigrams))\n",
        "\n",
        "  def unsmoothed_proba(self, s):\n",
        "    s = s.lower()\n",
        "    words = s.split()\n",
        "    return self.bigrams[s]/self.unigrams[words[0]]"
      ],
      "metadata": {
        "id": "spPwRUYfi2r5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    '<s> i am sam </s>',\n",
        "    '<s> sam i am </s>',\n",
        "    '<s> i am sam </s>',\n",
        "    '<s> i do not like green eggs and sam </s>',\n",
        "]\n",
        "\n",
        "model = BigramModel(corpus=corpus)\n",
        "print('Answer: ', round(model.add1smoothing_proba('am Sam'), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XlSBp6kmb4D",
        "outputId": "f985b204-d74b-479d-8c94-e7fe5b53186a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  0.214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Suppose we didn't use the end-symbol \\<\\s>. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol \\<\\s>:\n",
        "\n",
        "    '<s> a b',\n",
        "    '<s> b b',\n",
        "    '<s> b a',\n",
        "    '<s> a a'"
      ],
      "metadata": {
        "id": "AzUni4fUp7KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    '<s> a b',\n",
        "    '<s> b b',\n",
        "    '<s> b a',\n",
        "    '<s> a a'\n",
        "]\n",
        "\n",
        "model = BigramModel(corpus=corpus)\n",
        "model.unsmoothed_proba('a a') + model.unsmoothed_proba('a b') + model.unsmoothed_proba('b b') + model.unsmoothed_proba('b a')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANABYHY4qnkR",
        "outputId": "03bd424f-cccf-4459-d6c8-5e74ef4b6b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains V word types. Express a formula for estimating $P(w3,w1,w2)$, where $w3$ is a word which follows the bigram $(w1,w2)$, in terms of various n-gram counts and V."
      ],
      "metadata": {
        "id": "IntqWohrwE3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$P_{add-one} (w3|w1w2) = \\frac{C(w1w2w3)+1}{C(w1w2)+V_2}$\n",
        "\n",
        ", where $V_2$ is the number of unique bigrams in the corpus"
      ],
      "metadata": {
        "id": "BX5ykyKlwhYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7 We are given the following corpus, modified from the one in the chapter:\n",
        "    '<s> i am sam </s>',\n",
        "    '<s> sam i am </s>',\n",
        "    '<s> i am sam </s>',\n",
        "    '<s> i do not like green eggs and sam </s>',\n",
        "\n",
        "If we use linear interpolation smoothing between a maximum-likelihood bigram\n",
        "model and a maximum-likelihood unigram model with $\\lambda_1 = \\frac{1}{2}$ and $\\lambda_2 = \\frac{1}{2}$ , what is $P(Sam|am)$? Include \\<s> and </s> in your counts just like any other token."
      ],
      "metadata": {
        "id": "YQDyNKlJzCuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "$\\hat P(Sam|am) = \\lambda_1*P(Sam) + \\lambda_2*P(am|Sam)$"
      ],
      "metadata": {
        "id": "xl1Grh2B0roU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_1 = 0.5\n",
        "lambda_2 = 0.5\n",
        "p_sam = 4/25\n",
        "p_amsam = 2/3\n",
        "p_interpolation = lambda_1*p_sam + lambda_2*p_amsam\n",
        "print('Answer: ', p_interpolation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VciWU2ruwaNU",
        "outputId": "1dddd030-7cfb-44db-c55c-ff5869fe3a93"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  0.41333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.8 Write a program to compute unsmoothed unigrams and bigrams."
      ],
      "metadata": {
        "id": "5BWM6khL1r6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class NgramExtractor():\n",
        "  def __init__(self, filename):\n",
        "    self.filename = filename\n",
        "    self.corpus = self.read_text()\n",
        "    self.unigrams = self.extract_unigrams()\n",
        "    self.bigrams = self.extract_bigrams()\n",
        "\n",
        "  def read_text(self):\n",
        "    with open(self.filename) as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    import re\n",
        "    corpus = []\n",
        "    for line in lines:\n",
        "      line = line.replace('\\n', '')\n",
        "      sentences = re.split('[.?!]\\s*', line)\n",
        "      for s in sentences:\n",
        "        if s != '':\n",
        "          corpus.extend(['<s> ' + s.lower().replace(',', '') + ' <\\s>'])\n",
        "    return corpus\n",
        "\n",
        "  def extract_unigrams(self):\n",
        "\n",
        "    lst = {}\n",
        "    for s in self.corpus:\n",
        "      words = s.split()\n",
        "      # appending unigrams\n",
        "      for w in words:\n",
        "        if w not in lst.keys():\n",
        "          lst[w] = 1\n",
        "        else:\n",
        "          lst[w] += 1\n",
        "    total = sum(lst.values())\n",
        "    for index, value in lst.items():\n",
        "      lst[index] = value/total\n",
        "    return lst\n",
        "\n",
        "  def extract_bigrams(self):\n",
        "    lst = {}\n",
        "    for s in self.corpus:\n",
        "      words = s.split()\n",
        "      # appending bigrams\n",
        "      cntr = 0\n",
        "      for i in range(len(words)-1):\n",
        "        cur_bigram = (words[cntr] + ' ' + words[cntr+1])\n",
        "        if cur_bigram not in lst:\n",
        "          lst[cur_bigram] = 1\n",
        "        else:\n",
        "          lst[cur_bigram] += 1\n",
        "        cntr += 1\n",
        "    total = sum(lst.values())\n",
        "    for index, value in lst.items():\n",
        "      lst[index] = value/total\n",
        "    return lst\n",
        "\n",
        "  def add1smoothing_proba(self, s):\n",
        "    s = s.lower()\n",
        "    words = s.split()\n",
        "    return (self.bigrams[s] + 1) / (self.unigrams[words[0]]+len(self.unigrams))\n",
        "\n",
        "  def unsmoothed_proba(self, s):\n",
        "    s = s.lower()\n",
        "    words = s.split()\n",
        "    return self.bigrams[s]/self.unigrams[words[0]]\n",
        "\n",
        "  def generate_random_sentence(self, length):\n",
        "    words = list(self.unigrams.keys())\n",
        "    words.remove('<s>')\n",
        "    words.remove('<\\s>')\n",
        "    return ' '.join(random.sample(words, length)) + '.'"
      ],
      "metadata": {
        "id": "zHMZwPgJ2XXl"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.9 Run your n-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?"
      ],
      "metadata": {
        "id": "-X__cIfocIOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'MurderOfTheUniverse.txt' #@param\n",
        "model1 = NgramExtractor(filename)\n",
        "model1.corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhrzTPXg1wda",
        "outputId": "95c7b84c-c112-4536-fa0c-669b27872f9a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s> as soon as the dust settles you can see <\\\\s>',\n",
              " '<s> a new world in place of where the old one had been <\\\\s>',\n",
              " '<s> your skin is crawling with dry crusted mud <\\\\s>',\n",
              " '<s> and your naked feet are wet in a pool of blood <\\\\s>',\n",
              " '<s> and the whistle of the wind in your ears is so loud <\\\\s>',\n",
              " '<s> that your memories have blown up in a mushroom cloud <\\\\s>',\n",
              " '<s> and as your eyes accommodate <\\\\s>',\n",
              " '<s> there appears by the meadow <\\\\s>',\n",
              " '<s> a brute like a bear with a long dark shadow <\\\\s>',\n",
              " '<s> and you violently shake over what you have seen <\\\\s>']"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "motu = pd.Series(model1.unigrams).sort_values(ascending=False).drop(index=['<s>', '<\\s>'])\n",
        "motu[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTfigqTcKVzY",
        "outputId": "9b2cf56d-37a3-4a0e-c9a8-4891884d065d"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "i            0.040387\n",
              "the          0.033866\n",
              "and          0.025452\n",
              "to           0.016828\n",
              "you          0.015355\n",
              "a            0.014304\n",
              "of           0.012200\n",
              "altered      0.011359\n",
              "my           0.010938\n",
              "balrog       0.009886\n",
              "feel         0.009676\n",
              "me           0.009466\n",
              "beast        0.009255\n",
              "don't        0.007993\n",
              "am           0.007783\n",
              "in           0.007783\n",
              "is           0.007783\n",
              "your         0.006942\n",
              "see          0.006521\n",
              "it           0.006521\n",
              "vomit        0.006100\n",
              "oh           0.005259\n",
              "no           0.005048\n",
              "alter        0.004628\n",
              "that         0.004417\n",
              "with         0.004207\n",
              "are          0.004207\n",
              "an           0.004207\n",
              "from         0.003997\n",
              "can          0.003786\n",
              "as           0.003576\n",
              "like         0.003576\n",
              "through      0.003155\n",
              "his          0.003155\n",
              "want         0.003155\n",
              "into         0.003155\n",
              "nonagon      0.002945\n",
              "for          0.002945\n",
              "he           0.002735\n",
              "coffin       0.002735\n",
              "all          0.002735\n",
              "will         0.002524\n",
              "lightning    0.002524\n",
              "up           0.002314\n",
              "pain         0.002314\n",
              "head         0.002314\n",
              "so           0.002314\n",
              "one          0.002314\n",
              "have         0.002103\n",
              "be           0.002103\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'NonagonInfinity.txt' #@param\n",
        "model2 = NgramExtractor(filename)\n",
        "model2.corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVqqxBQmNgll",
        "outputId": "3e269434-c64d-4224-ca5e-5de3a8b7a0ad"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s> nonagon infinity opens the door <\\\\s>',\n",
              " '<s> nonagon infinity opens the door <\\\\s>',\n",
              " '<s> wait for the answer to open the door <\\\\s>',\n",
              " '<s> nonagon infinity opens the door <\\\\s>',\n",
              " '<s> one two three <\\\\s>',\n",
              " '<s> loosen up <\\\\s>',\n",
              " '<s> time to drop <\\\\s>',\n",
              " '<s> fuck shit up <\\\\s>',\n",
              " \"<s> don't forget about it <\\\\s>\",\n",
              " \"<s> my coffin's all i see <\\\\s>\"]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nonagon = pd.Series(model2.unigrams).sort_values(ascending=False).drop(index=['<s>', '<\\s>'])\n",
        "nonagon[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_9aeZjtNpxL",
        "outputId": "ece036f3-06b1-4454-cb5c-beab2cd023e1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "the          109\n",
              "a             51\n",
              "i             46\n",
              "my            36\n",
              "it's          35\n",
              "beat          33\n",
              "to            30\n",
              "i'm           27\n",
              "door          25\n",
              "wasp          25\n",
              "of            24\n",
              "gamma         24\n",
              "is            23\n",
              "knife         22\n",
              "infinity      22\n",
              "nonagon       22\n",
              "only          21\n",
              "fig           21\n",
              "and           20\n",
              "it            18\n",
              "you           18\n",
              "miss          18\n",
              "for           17\n",
              "mr            16\n",
              "once          16\n",
              "opens         15\n",
              "see           15\n",
              "up            14\n",
              "know          14\n",
              "wah           12\n",
              "face          12\n",
              "what          11\n",
              "when          10\n",
              "body          10\n",
              "here           9\n",
              "can            9\n",
              "your           9\n",
              "again          9\n",
              "all            9\n",
              "in             9\n",
              "has            9\n",
              "universe       8\n",
              "have           8\n",
              "invisible      8\n",
              "down           8\n",
              "robot          8\n",
              "same           8\n",
              "just           8\n",
              "roll           7\n",
              "machine        7\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cntr = 0\n",
        "print('Answer: ')\n",
        "for t in model.bigrams.keys():\n",
        "  if cntr < 10:\n",
        "    # if t.startswith('you '):\n",
        "    print(f'P({t}) = {model.unsmoothed_proba(t)}')\n",
        "    cntr +=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4aSZbUtC01G",
        "outputId": "bd3eb036-7938-46cc-c928-7255440fef3e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: \n",
            "P(<s> as) = 0.008875739644970414\n",
            "P(as soon) = 0.058823529411764705\n",
            "P(soon as) = 1.0\n",
            "P(as the) = 0.35294117647058826\n",
            "P(the dust) = 0.006211180124223602\n",
            "P(dust settles) = 0.5\n",
            "P(settles you) = 1.0\n",
            "P(you can) = 0.0273972602739726\n",
            "P(can see) = 0.3888888888888889\n",
            "P(see <\\s>) = 0.22580645161290322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Not really the best or even suitable material for this excercise but still better than nothing. Initially was planning to use happy new year messages published by some independent russian media in 2020, 2021 vs. 2022, 2023, because i thought they would show the shift of the focus from the coronavirus in 2020-2021 to the war in 2022-2023. But sadly i didn't find any media that published such messages for 4 years in a row. So instead settled for lyrics from two King Gizzard and the Lizzard Wizard albums -- Murder of the Universe and Nonagon Infinity.\n",
        "\n",
        "Both showed the highest frequencies for the articles 'the' and 'a' and pronouns like 'i', 'me' and 'my'. In the first album, among most used words are those associated with repulsiveness and evil, such as 'beast', 'balrog', 'vomit', 'pain', 'coffin' etc. which goes hand in hand with the album's narrative."
      ],
      "metadata": {
        "id": "HSzyx3nIdgcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.10 Add an option to your program to generate random sentences."
      ],
      "metadata": {
        "id": "WHW6Iu9cmZVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a random sentence of 10 words\n",
        "model1.generate_random_sentence(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SWE4pG84mVAZ",
        "outputId": "fe2629ee-003c-41bf-8bfb-737794590dbb"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'swathe deepest heave sabre life lugubrious slip a rotten soggy.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a random sentence of 10 words\n",
        "model2.generate_random_sentence(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "inGF6ToMlIF4",
        "outputId": "4f81ff31-f8f8-4c25-82f9-12dfcc9834f8"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'limbs dream dueling rubber obliteration while winged of aflame fig.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.11 Add an option to your program to compute the perplexity of a test set."
      ],
      "metadata": {
        "id": "QBHwreIImeQL"
      }
    }
  ]
}