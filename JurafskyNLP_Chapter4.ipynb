{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy3PjX67/8Q032l5ctDS7E"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Assume the following likelihoods for each word being part of a positive or negative movie review, and equal prior probabilities for each class.\n",
        "```\n",
        "      positive \tnegative\n",
        "I       0.09 \t  0.16\n",
        "always  0.07 \t  0.06\n",
        "like    0.29 \t  0.06\n",
        "foreign 0.04 \t  0.15\n",
        "films   0.08 \t  0.11\n",
        "```\n",
        "\n",
        "\n",
        "What class will Naive bayes assign to the sentence “I always like foreign\n",
        "films.”?"
      ],
      "metadata": {
        "id": "qp_2U7KAqQBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "d = {\n",
        "    'pos': 0.5 * np.prod([0.09, 0.07, 0.29, 0.04, 0.08]),\n",
        "    'neg': 0.5 * np.prod([0.16, 0.06, 0.06, 0.15, 0.11])\n",
        "}\n",
        "max(d, key=d.get)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xDTYix8Ir3Ct",
        "outputId": "1274f6d5-c8f5-4ba7-e545-840bc7eb13d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Given the following short movie reviews, each labeled with a genre, either comedy or action:\n",
        "```\n",
        "1. fun, couple, love, love      comedy\n",
        "2. fast, furious, shoot         action\n",
        "3. couple, fly, fast, fun, fun  comedy\n",
        "4. furious, shoot, shoot, fun   action\n",
        "5. fly, fast, shoot, love       action\n",
        "```\n",
        "and a new document D:\n",
        "\n",
        "`fast, couple, shoot, fly`\n",
        "\n",
        "compute the most likely class for D. Assume a naive Bayes classifier and use\n",
        "add-1 smoothing for the likelihoods."
      ],
      "metadata": {
        "id": "-FhKGrTxtLfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "corpus = [\n",
        "    (['fun', 'couple', 'love', 'love'], 'comedy'),\n",
        "    (['couple', 'fly', 'fast', 'fun', 'fun'], 'comedy'),\n",
        "    (['fast', 'furious', 'shoot'], 'action'),\n",
        "    (['furious', 'shoot', 'shoot', 'fun'], 'action'),\n",
        "    (['fly', 'fast', 'shoot', 'love'], 'action'),\n",
        "]\n",
        "\n",
        "def train_naive_bayes(crps, cls):\n",
        "  res = {\n",
        "      'V': None,\n",
        "  }\n",
        "  for c in cls:\n",
        "    res[c] = {\n",
        "          'logprior': 0,\n",
        "          'words': {}\n",
        "      }\n",
        "\n",
        "  n_doc = len(corpus) # total number of documents in the training set\n",
        "\n",
        "  vocab = set() # number of word types (i.e. unique lemmas)\n",
        "  res['V'] = vocab\n",
        "  for d in corpus:\n",
        "    vocab.update(d[0])\n",
        "\n",
        "  for c in cls:\n",
        "    docs_cls = list(filter(lambda x: x[1]==c, crps)) # documents in class c\n",
        "    n_c = len(docs_cls) # number of documents in class c\n",
        "    logprior = log(n_c/n_doc, 10) # percentage of the documents in our training set that are in each class c\n",
        "    res[c]['logprior'] = logprior\n",
        "\n",
        "    bigdoc = [] # all tokens in class c\n",
        "    for dc in docs_cls:\n",
        "      bigdoc.extend(dc[0])\n",
        "\n",
        "    for w in vocab: # for each word type (i.e. unique lemma)\n",
        "      n_occurences_cls = bigdoc.count(w)\n",
        "      loglikelihood = log((n_occurences_cls+1)/(len(bigdoc)+len(vocab)), 10)\n",
        "      res[c]['words'][w] = loglikelihood\n",
        "\n",
        "  return res\n",
        "\n",
        "model = train_naive_bayes(corpus, ['comedy', 'action'])\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE2LQ5OL1QvC",
        "outputId": "ff45ba4d-bbfe-4879-9628-6c19eee96d7c"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'V': {'couple', 'fast', 'fly', 'fun', 'furious', 'love', 'shoot'},\n",
              " 'comedy': {'logprior': -0.39794000867203755,\n",
              "  'words': {'love': -0.7269987279362623,\n",
              "   'fast': -0.9030899869919434,\n",
              "   'fly': -0.9030899869919434,\n",
              "   'fun': -0.6020599913279623,\n",
              "   'shoot': -1.2041199826559246,\n",
              "   'furious': -1.2041199826559246,\n",
              "   'couple': -0.7269987279362623}},\n",
              " 'action': {'logprior': -0.22184874961635637,\n",
              "  'words': {'love': -0.9542425094393249,\n",
              "   'fast': -0.7781512503836435,\n",
              "   'fly': -0.9542425094393249,\n",
              "   'fun': -0.9542425094393249,\n",
              "   'shoot': -0.5563025007672872,\n",
              "   'furious': -0.7781512503836435,\n",
              "   'couple': -1.255272505103306}}}"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_naive_bayes(test_doc, mdl, cls):\n",
        "  if type(test_doc) == str:\n",
        "   test_doc = test_doc.split()\n",
        "  p = {}\n",
        "  for c in cls:\n",
        "    p[c] = mdl[c]['logprior']\n",
        "    for w in test_doc:\n",
        "      if w in mdl['V']:\n",
        "        p[c] += mdl[c]['words'][w]\n",
        "    print(c, p[c])\n",
        "  return max(p, key=p.get)\n",
        "\n",
        "test_naive_bayes(['fast', 'couple', 'shoot', 'fly'], model, ['comedy', 'action'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "VBKm0LsNIman",
        "outputId": "cd2ad920-9a0b-4867-bd3e-e02e866bb6d1"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comedy -4.1352386932481116\n",
            "action -3.765817515309918\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'action'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Train two models, multinomial naive Bayes and binarized naive Bayes, both with add-1 smoothing, on the following document counts for key sentiment words, with positive or negative class assigned as noted.\n",
        "\n",
        "```\n",
        "doc   “good”  “poor”  “great” (class)\n",
        "d1.   3       0       3       pos\n",
        "d2.   0       1       2       pos\n",
        "d3.   1       3       0       neg\n",
        "d4.   1       5       2       neg\n",
        "d5.   0       2       0       neg\n",
        "```\n",
        "\n",
        "Use both naive Bayes models to assign a class (pos or neg) to this sentence:\n",
        "\n",
        "`A good, good plot and great characters, but poor acting.`"
      ],
      "metadata": {
        "id": "uloTMVSmtp6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# multinomial naive Bayes\n",
        "corpus = [\n",
        "    (['good', 'good', 'good', 'great','great', 'great',], 'pos'),\n",
        "    (['poor', 'great', 'great',], 'pos'),\n",
        "    (['good', 'poor', 'poor', 'poor'], 'neg'),\n",
        "    (['good', 'poor', 'poor', 'poor','poor','poor','great', 'great'], 'neg'),\n",
        "    (['poor', 'poor'], 'neg'),\n",
        "\n",
        "]\n",
        "model = train_naive_bayes(corpus, ['pos', 'neg'])\n",
        "print(model)\n",
        "test_naive_bayes('A good, good plot and great characters, but poor acting.', model,  ['pos', 'neg'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "DSfT48tGy52V",
        "outputId": "09bf419b-66fe-4d5c-9a43-1340194d5ed0"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'V': {'good', 'great', 'poor'}, 'pos': {'logprior': -0.39794000867203755, 'words': {'good': -0.47712125471966244, 'great': -0.30102999566398114, 'poor': -0.7781512503836435}}, 'neg': {'logprior': -0.22184874961635637, 'words': {'good': -0.7533276666586114, 'great': -0.7533276666586114, 'poor': -0.18905623622004886}}}\n",
            "pos -1.9542425094393248\n",
            "neg -1.917560319153628\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# binarized naive Bayes\n",
        "from math import log\n",
        "def train_naive_bayes_binarized(crps, cls):\n",
        "  res = {\n",
        "      'V': None,\n",
        "  }\n",
        "  for c in cls:\n",
        "    res[c] = {\n",
        "          'logprior': 0,\n",
        "          'words': {}\n",
        "      }\n",
        "\n",
        "  n_doc = len(corpus) # total number of documents in the training set\n",
        "\n",
        "  vocab = set() # number of word types (i.e. unique lemmas)\n",
        "  res['V'] = vocab\n",
        "  for d in corpus:\n",
        "    vocab.update(d[0])\n",
        "\n",
        "  for c in cls:\n",
        "    docs_cls = [] # documents in class c\n",
        "    for d in crps:\n",
        "      if d[1] == c:\n",
        "        docs_cls.append((list(set(d[0])),d[1]))\n",
        "    n_c = len(docs_cls) # number of documents in class c\n",
        "    logprior = log(n_c/n_doc, 10) # percentage of the documents in our training set that are in each class c\n",
        "    res[c]['logprior'] = logprior\n",
        "\n",
        "    bigdoc = [] # all tokens in class c\n",
        "    for dc in docs_cls:\n",
        "      bigdoc.extend(dc[0])\n",
        "\n",
        "    for w in vocab: # for each word type (i.e. unique lemma)\n",
        "      n_occurences_cls = bigdoc.count(w)\n",
        "      loglikelihood = log((n_occurences_cls+1)/(len(bigdoc)+len(vocab)), 10)\n",
        "      res[c]['words'][w] = loglikelihood\n",
        "\n",
        "  return res\n",
        "\n",
        "model = train_naive_bayes_binarized(corpus, ['pos', 'neg'])\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z6Mu5vyVyim",
        "outputId": "5b27a410-6588-475a-f8da-bfba16c26a14"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'V': {'good', 'great', 'poor'},\n",
              " 'pos': {'logprior': -0.39794000867203755,\n",
              "  'words': {'good': -0.5440680443502756,\n",
              "   'great': -0.3679767852945944,\n",
              "   'poor': -0.5440680443502756}},\n",
              " 'neg': {'logprior': -0.22184874961635637,\n",
              "  'words': {'good': -0.47712125471966244,\n",
              "   'great': -0.6532125137753436,\n",
              "   'poor': -0.3521825181113625}}}"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_naive_bayes_binarized(test_doc, mdl, cls):\n",
        "  if type(test_doc) == str:\n",
        "   test_doc = set(test_doc.split())\n",
        "  p = {}\n",
        "  for c in cls:\n",
        "    p[c] = mdl[c]['logprior']\n",
        "    for w in test_doc:\n",
        "      if w in mdl['V']:\n",
        "        p[c] += mdl[c]['words'][w]\n",
        "    print(c, p[c])\n",
        "  return max(p, key=p.get)\n",
        "\n",
        "test_naive_bayes_binarized('A good, good plot and great characters, but poor acting.', model,  ['pos', 'neg'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "PPJtAAeeXxiO",
        "outputId": "3ad530f8-a8c6-4f07-f005-f6a53d76f9d8"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos -1.8540528826671832\n",
            "neg -1.704365036222725\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    }
  ]
}